{"version":3,"sources":["music/mymusiccut.mp3","App.js","reportWebVitals.js","index.js"],"names":["sound","Howl","src","soundURL","TOUCHED","App","video","useRef","classifier","canPlaySound","mobinetModule","useState","touched","setTouched","init","a","console","log","setupCamera","current","knnClassifier","mobilenet","initNotifications","cooldown","Promise","resolve","reject","navigator","getUserMedia","webkitGetUserMedia","mozGetUserMedia","msGetUserMedia","stream","srcObject","addEventListener","error","train","label","i","progress","parseInt","training","embedding","infer","addExample","sleep","run","predictClass","result","confidences","play","notify","body","ms","setTimeout","useEffect","on","className","ref","autoPlay","onClick","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"uaAAe,MAA0B,uC,QCUrCA,EAAQ,IAAIC,OAAK,CACnBC,IAAK,CAACC,KAIFC,EAAU,UAkIDC,MA7Hf,WACE,IAAMC,EAAQC,mBACRC,EAAaD,mBACbE,EAAeF,kBAAO,GACtBG,EAAgBH,mBACtB,EAA6BI,oBAAS,GAAtC,mBAAOC,EAAP,KAAeC,EAAf,KAEMC,EAAI,uCAAG,sBAAAC,EAAA,6DACXC,QAAQC,IAAI,UADD,SAELC,IAFK,cAGXF,QAAQC,IAAI,kBAEZT,EAAWW,QAAUC,MALV,SAOmBC,MAPnB,OAOXX,EAAcS,QAPH,OASXH,QAAQC,IAAI,gEAEZK,YAAkB,CAAEC,SAAU,MAXnB,4CAAH,qDAeJL,EAAc,WAClB,OAAO,IAAIM,SAAQ,SAACC,EAASC,GAC3BC,UAAUC,aAAeD,UAAUC,cACnCD,UAAUE,oBACVF,UAAUG,iBACVH,UAAUI,eAEPJ,UAAUC,aACXD,UAAUC,aACR,CAACtB,OAAM,IACP,SAAA0B,GACE1B,EAAMa,QAAQc,UAAYD,EAC1B1B,EAAMa,QAAQe,iBAAiB,aAAaT,MAE9C,SAAAU,GAAK,OAAIT,EAAOS,MAGlBT,QAKAU,EAAK,uCAAG,WAAMC,GAAN,iBAAAtB,EAAA,sDACZC,QAAQC,IAAR,UAAeoB,EAAf,gEACQC,EAAI,EAFA,YAEGA,EAlDI,IAgDP,wBAGNC,EAAW,YAAcC,UAAUF,EAAE,GAnDxB,GAmD8C,KAAM,IACrEtB,QAAQC,IAAIsB,GAJF,SAMJE,EAASJ,GANL,OAEuBC,IAFvB,2DAAH,sDAULG,EAAW,SAAAJ,GACf,OAAO,IAAIb,QAAJ,uCAAY,WAAMC,GAAN,eAAAV,EAAA,6DACX2B,EAAYhC,EAAcS,QAAQwB,MACtCrC,EAAMa,SACN,GAEFX,EAAWW,QAAQyB,WAAWF,EAAUL,GALvB,SAMXQ,EAAM,KANK,OAOjBpB,IAPiB,2CAAZ,wDAYHqB,EAAG,uCAAG,8BAAA/B,EAAA,6DACJ2B,EAAYhC,EAAcS,QAAQwB,MACtCrC,EAAMa,SACN,GAHQ,SAKWX,EAAWW,QAAQ4B,aAAaL,GAL3C,cAKJM,EALI,QASAX,QAAUjC,GAAW4C,EAAOC,YAAYD,EAAOX,OA/ErC,IAgFlBrB,QAAQC,IAAI,WACTR,EAAaU,UACdV,EAAaU,SAAU,EACvBnB,EAAMkD,QAERC,YAAO,iBAAa,CAAEC,KAAM,sDAC5BvC,GAAW,KAEXG,QAAQC,IAAI,eACZJ,GAAW,IAnBH,SAsBJgC,EAAM,KAtBF,OAwBVC,IAxBU,2CAAH,qDA2BHD,EAAQ,SAAAQ,GACZ,OAAO,IAAI7B,SAAQ,SAAAC,GAAO,OAAI6B,WAAW7B,EAAQ4B,OAWnD,OARAE,qBAAU,WACRzC,IAEAd,EAAMwD,GAAG,OAAO,WACd/C,EAAaU,SAAU,OAEzB,IAGA,sBAAKsC,UAAS,eAAU7C,EAAU,UAAY,IAA9C,UAEE,uBACA8C,IAAKpD,EACLmD,UAAU,QACVE,UAAQ,IAGR,sBAAKF,UAAU,UAAf,UACE,wBAAQA,UAAU,MAAMG,QAAS,kBAAIxB,EA1H3B,cA0HV,qBACA,wBAAQqB,UAAU,MAAMG,QAAS,kBAAIxB,EAAMhC,IAA3C,qBACA,wBAAQqD,UAAU,MAAMG,QAAS,kBAAId,KAArC,4BC9HOe,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.0745db3c.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/mymusiccut.1bc029c5.mp3\";","import { useEffect, useRef, useState } from 'react';\nimport './App.css';\nimport * as mobilenet from '@tensorflow-models/mobilenet';\nimport * as knnClassifier from '@tensorflow-models/knn-classifier';\nimport '@tensorflow/tfjs-backend-cpu';\nimport * as tf from '@tensorflow/tfjs';\nimport { Howl } from 'howler';\nimport { initNotifications, notify } from '@mycv/f8-notification';\nimport soundURL from './music/mymusiccut.mp3';\n\nvar sound = new Howl({\n  src: [soundURL]\n});\n\nconst NOT_TOUCH = 'not_touch';\nconst TOUCHED = 'touched';\nconst TRAINING_TIMES = 50;\nconst TOUCH_CONFIDENT = 0.7;\n\n\nfunction App() {\n  const video = useRef();\n  const classifier = useRef();\n  const canPlaySound = useRef(true);\n  const mobinetModule = useRef();\n  const [touched,setTouched] = useState(false);\n\n  const init = async () => {\n    console.log('init..');\n    await setupCamera();\n    console.log('success camera');\n\n    classifier.current = knnClassifier.create();\n\n    mobinetModule.current = await mobilenet.load();\n\n    console.log('Không chạm tay lên mặt và ấn Train 1');\n\n    initNotifications({ cooldown: 3000 });\n\n  }\n\n  const setupCamera = () => {\n    return new Promise((resolve, reject) => {\n      navigator.getUserMedia = navigator.getUserMedia || \n      navigator.webkitGetUserMedia ||\n      navigator.mozGetUserMedia ||\n      navigator.msGetUserMedia;\n\n      if(navigator.getUserMedia){\n        navigator.getUserMedia(\n          {video:true},\n          stream => {\n            video.current.srcObject = stream;\n            video.current.addEventListener('loadeddata',resolve);\n          },\n          error => reject(error)\n        );\n      }else{\n        reject();\n      }\n    });\n  }\n\n  const train = async label => {\n    console.log(`${label} Đang train cho khuôn mặt của bạn...`);\n    for(let i = 0; i < TRAINING_TIMES; i++){\n      let progress = \"Progress \" + parseInt((i+1) / TRAINING_TIMES * 100) +\"%\";\n      console.log(progress);\n \n      await training(label);\n    }\n  }\n\n  const training = label => {\n    return new Promise(async resolve => {\n      const embedding = mobinetModule.current.infer(\n        video.current,\n        true\n      );\n      classifier.current.addExample(embedding,label);\n      await sleep(100);\n      resolve();\n      \n    })\n  }\n\n  const run = async () => {\n    const embedding = mobinetModule.current.infer(\n      video.current,\n      true\n    );\n    const result = await classifier.current.predictClass(embedding);\n    // console.log('Label: ', result.label);\n    // console.log('Confidences: ', result.confidences);\n\n    if(result.label === TOUCHED && result.confidences[result.label] > TOUCH_CONFIDENT){\n      console.log('Touched');\n      if(canPlaySound.current){\n        canPlaySound.current = false;\n        sound.play();\n      }\n      notify('Bỏ tay ra', { body: 'Bạn vừa chạm tay vào mặt!!' });\n      setTouched(true);\n    }else{\n      console.log('Not_touched');\n      setTouched(false);\n    }\n\n    await sleep(200);\n\n    run();\n  }\n\n  const sleep = ms => {\n    return new Promise(resolve => setTimeout(resolve,ms))\n  }\n\n  useEffect(()=> {\n    init();\n\n    sound.on('end', function(){\n      canPlaySound.current = true;\n    });\n  },[])\n\n  return (\n    <div className={`main ${touched ? \"touched\" : \"\"}`}>\n\n      <video\n      ref={video} \n      className=\"video\"\n      autoPlay \n      />\n\n      <div className=\"control\">\n        <button className=\"btn\" onClick={()=>train(NOT_TOUCH)}>Train 1</button>\n        <button className=\"btn\" onClick={()=>train(TOUCHED)}>Train 2</button>\n        <button className=\"btn\" onClick={()=>run()}>Train 3</button>\n      </div>\n\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}